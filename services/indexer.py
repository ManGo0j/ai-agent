import os
import glob
import uuid
import hashlib
import pdfplumber
import asyncio
from docx import Document
from datetime import datetime
from typing import List

from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è SQLAlchemy 2.0
from sqlalchemy import select, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker

# –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–µ–π –ë–î
from database.models import AdminDocument, DocumentChunk

# –ò–º–ø–æ—Ä—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DOCS_DIR = os.path.join(BASE_DIR, "docs")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Qdrant –∏ –ë–î
QDRANT_URL = os.getenv("QDRANT_URL", "http://accountant_qdrant:6333")
DATABASE_URL = os.getenv("DATABASE_URL")
COLLECTION_NAME = "knowledge_base"
MODEL_NAME = "all-MiniLM-L6-v2" 

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
CHUNK_SIZE = 2000 
CHUNK_OVERLAP = 300 

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ---
qdrant_client = QdrantClient(url=QDRANT_URL)
model = SentenceTransformer(MODEL_NAME)
engine = create_async_engine(DATABASE_URL)

# –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–±—Ä–∏–∫–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π
AsyncSessionLocal = async_sessionmaker(
    bind=engine, 
    class_=AsyncSession, 
    expire_on_commit=False
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    separators=["\n\n", "\n", " ", ""]
)

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def get_file_hash(file_path: str) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã."""
    hasher = hashlib.sha256()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hasher.update(chunk)
    return hasher.hexdigest()

def extract_text_from_docx(file_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ Word."""
    try:
        doc = Document(file_path)
        return "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX {file_path}: {e}")
        return ""

def extract_text_from_pdf(file_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF."""
    text = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {file_path}: {e}")
        return ""

async def process_files():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞/–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
    try:
        if not qdrant_client.collection_exists(COLLECTION_NAME):
            qdrant_client.create_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)
            )
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è Qdrant: {COLLECTION_NAME}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}")
        return

    # 2. –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    files = []
    extensions = ['*.pdf', '*.docx', '*.doc', '*.txt']
    for ext in extensions:
        files.extend(glob.glob(os.path.join(DOCS_DIR, ext)))

    if not files:
        print(f"‚ö†Ô∏è –§–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {DOCS_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(files)}")

    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    async with AsyncSessionLocal() as session:
        for file_path in files:
            file_name = os.path.basename(file_path)
            
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ö–µ—à–∞
                file_hash = get_file_hash(file_path)
                result = await session.execute(
                    select(AdminDocument).where(AdminDocument.file_hash == file_hash)
                )
                if result.scalar_one_or_none():
                    print(f"‚è© –ü—Ä–æ–ø—É—Å–∫: {file_name} —É–∂–µ –≤ –±–∞–∑–µ.")
                    continue

                print(f"üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_name}...")

                # –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
                text_content = ""
                doc_type = "–¥–æ–∫—É–º–µ–Ω—Ç"
                
                if file_path.lower().endswith('.pdf'):
                    text_content = extract_text_from_pdf(file_path)
                    doc_type = "scan/pdf"
                elif file_path.lower().endswith(('.docx', '.doc')):
                    text_content = extract_text_from_docx(file_path)
                    doc_type = "word"
                elif file_path.lower().endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        text_content = f.read()

                if not text_content.strip():
                    print(f"‚ö†Ô∏è –§–∞–π–ª {file_name} –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –ø—Ä–æ—á–∏—Ç–∞–Ω.")
                    continue

                # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
                db_doc = AdminDocument(
                    document_name=file_name,
                    file_path=file_path,
                    file_hash=file_hash,
                    document_type=doc_type,
                    upload_date=datetime.utcnow()
                )
                session.add(db_doc)
                await session.flush() 

                # 4. –ß–∞–Ω–∫–∏–Ω–≥ –∏ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
                chunks = text_splitter.split_text(text_content)
                embeddings = model.encode(chunks)

                points = []
                for i, (chunk_text, vector) in enumerate(zip(chunks, embeddings)):
                    point_id = str(uuid.uuid4())
                    
                    db_chunk = DocumentChunk(
                        document_id=db_doc.id,
                        chunk_index=i,
                        chunk_text=chunk_text,
                        embedding_id=point_id
                    )
                    session.add(db_chunk)

                    points.append(models.PointStruct(
                        id=point_id,
                        vector=vector.tolist(),
                        payload={
                            "document_id": db_doc.id,
                            "document_name": file_name,
                            "text": chunk_text
                        }
                    ))

                # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant
                if points:
                    for k in range(0, len(points), 100):
                        qdrant_client.upsert(
                            collection_name=COLLECTION_NAME, 
                            points=points[k : k + 100]
                        )
                
                await session.commit()
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {file_name} (—Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤)")

            except Exception as e:
                await session.rollback()
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å —Ñ–∞–π–ª–æ–º {file_name}: {str(e)}")
                continue

if __name__ == "__main__":
    asyncio.run(process_files())