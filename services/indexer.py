import os
import glob
import uuid
import hashlib
import pdfplumber
import asyncio
from docx import Document
from datetime import datetime
from typing import List

from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http import models

from sqlalchemy import select, text
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker

# –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–µ–π –ë–î
from database.models import AdminDocument, DocumentChunk

# –ò–º–ø–æ—Ä—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
from langchain_text_splitters import RecursiveCharacterTextSplitter

load_dotenv()

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DOCS_DIR = os.path.join(BASE_DIR, "docs")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Qdrant –∏ –ë–î
QDRANT_URL = os.getenv("QDRANT_URL", "http://accountant_qdrant:6333")
DATABASE_URL = os.getenv("DATABASE_URL")
COLLECTION_NAME = "knowledge_base"
MODEL_NAME = "all-MiniLM-L6-v2" # –õ–µ–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û)
# –£–≤–µ–ª–∏—á–∏–ª–∏ —Ä–∞–∑–º–µ—Ä, —á—Ç–æ–±—ã —Ç–∞–±–ª–∏—Ü—ã –∏ —Å–ø–∏—Å–∫–∏ –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞–ª–∏—Å—å
CHUNK_SIZE = 2000 
CHUNK_OVERLAP = 300 

# --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ---
qdrant_client = QdrantClient(url=QDRANT_URL)
model = SentenceTransformer(MODEL_NAME)
engine = create_async_engine(DATABASE_URL)

AsyncSessionLocal = async_sessionmaker(
    bind=engine, 
    class_=AsyncSession, 
    expire_on_commit=False
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –æ–¥–∏–Ω —Ä–∞–∑
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP,
    separators=["\n\n", "\n", " ", ""] # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ä–∞–∑—Ä—ã–≤–∞: –∞–±–∑–∞—Ü -> —Å—Ç—Ä–æ–∫–∞ -> –ø—Ä–æ–±–µ–ª
)

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ---

def get_file_hash(file_path: str) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã."""
    hasher = hashlib.sha256()
    with open(file_path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hasher.update(chunk)
    return hasher.hexdigest()

def extract_text_from_docx(file_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ Word."""
    try:
        doc = Document(file_path)
        return "\n".join([para.text for para in doc.paragraphs])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è DOCX {file_path}: {e}")
        return ""

def extract_text_from_pdf(file_path: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF."""
    text = ""
    try:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {file_path}: {e}")
        return ""



async def process_files():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞/–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
    if not qdrant_client.collection_exists(COLLECTION_NAME):
        qdrant_client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE)
        )
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è Qdrant: {COLLECTION_NAME}")

    # 2. –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ (–¥–æ–±–∞–≤–ª–µ–Ω RTF)
    files = []
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º–∞—Ç–æ–≤
    extensions = ['*.pdf', '*.docx', '*.doc', '*.rtf', '*.txt']
    for ext in extensions:
        files.extend(glob.glob(os.path.join(DOCS_DIR, ext)))

    if not files:
        print(f"‚ö†Ô∏è –§–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {DOCS_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(files)}")

    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    async with AsyncSessionLocal() as session:
        for file_path in files:
            file_name = os.path.basename(file_path)
            
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ö–µ—à–∞ (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
                file_hash = get_file_hash(file_path)
                result = await session.execute(
                    select(AdminDocument).where(AdminDocument.file_hash == file_hash)
                )
                if result.scalar_one_or_none():
                    print(f"‚è© –ü—Ä–æ–ø—É—Å–∫: {file_name} —É–∂–µ –≤ –±–∞–∑–µ.")
                    continue

                print(f"üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_name}...")

                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏ —á—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
                text = ""
                doc_type = "–¥–æ–∫—É–º–µ–Ω—Ç"
                
                if file_path.lower().endswith('.pdf'):
                    text = extract_text_from_pdf(file_path)
                    doc_type = "—Å–∫–∞–Ω/pdf"
                elif file_path.lower().endswith('.docx') or file_path.lower().endswith('.doc'):
                    text = extract_text_from_docx(file_path)
                    doc_type = "–¥–æ–∫—É–º–µ–Ω—Ç Word"
                elif file_path.lower().endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()

                if not text.strip():
                    print(f"‚ö†Ô∏è –§–∞–π–ª {file_name} –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –ø—Ä–æ—á–∏—Ç–∞–Ω.")
                    continue

                # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤ Postgres
                db_doc = AdminDocument(
                    document_name=file_name,
                    file_path=file_path,
                    file_hash=file_hash,
                    document_type=doc_type,
                    upload_date=datetime.utcnow()
                )
                session.add(db_doc)
                await session.flush() # –ü–æ–ª—É—á–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–∞

                # 4. –£–º–Ω—ã–π –ß–∞–Ω–∫–∏–Ω–≥
                chunks = text_splitter.split_text(text)
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ (—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
                embeddings = model.encode(chunks)

                points = []
                for i, (chunk_text, vector) in enumerate(zip(chunks, embeddings)):
                    point_id = str(uuid.uuid4())
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ –≤ Postgres
                    db_chunk = DocumentChunk(
                        document_id=db_doc.id,
                        chunk_index=i,
                        chunk_text=chunk_text,
                        embedding_id=point_id
                    )
                    session.add(db_chunk)

                    # –ì–æ—Ç–æ–≤–∏–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è Qdrant
                    points.append(models.PointStruct(
                        id=point_id,
                        vector=vector.tolist(),
                        payload={
                            "document_id": db_doc.id,
                            "document_name": file_name,
                            "type": doc_type,
                            "text": chunk_text # –î—É–±–ª–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –≤ payload –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                        }
                    ))

                # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant –ø–∞—á–∫–∞–º–∏
                if points:
                    batch_size = 100
                    for k in range(0, len(points), batch_size):
                        batch = points[k : k + batch_size]
                        qdrant_client.upsert(collection_name=COLLECTION_NAME, points=batch)
                
                await session.commit()
                print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {file_name} (—Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤)")

            except Exception as e:
                await session.rollback()
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å —Ñ–∞–π–ª–æ–º {file_name}: {str(e)}")
                continue

if __name__ == "__main__":
    asyncio.run(process_files())